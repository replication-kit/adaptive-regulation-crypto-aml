# -*- coding: utf-8 -*-
"""TableC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/174ZfD2IpvNHtWNUwrqL5wLMp7TJPqaxm
"""

import numpy as np

# -----------------------------
# Utilities
# -----------------------------
def clip01(x):
    return np.minimum(1.0, np.maximum(0.0, x))

# -----------------------------
# Core simulation
# -----------------------------
def simulate_metrics(
    Delta,
    sigma,
    update_rule="baseline",   # "baseline" or "smoothed"
    rho=0.3,                  # smoothing parameter
    J=10,
    T=1200,
    burn=300,
    theta=1.0,
    Dbar=0.5,
    lam=5.0,
    Mbar=1.0,
    tau=0.25,
    seed=0
):
    rng = np.random.default_rng(seed)

    s = rng.uniform(0, 1, size=J)
    M = np.full(J, Mbar / J)

    gaps = []

    for t in range(T):
        D = theta * s
        Dhat = D + rng.normal(0.0, sigma, size=J)

        # baseline discrete update
        s_target = clip01(
            s
            + Delta * (Dhat < Dbar)
            - Delta * (Dhat > Dbar)
        )

        # smoothed update
        if update_rule == "smoothed":
            s = clip01((1 - rho) * s + rho * s_target)
        else:
            s = s_target

        # illicit fund relocation
        w = np.exp(-lam * D)
        M = Mbar * w / w.sum()

        gaps.append(D.max() - D.min())

    gaps = np.array(gaps[burn:])

    lockin_index = gaps.mean()
    severe_prob = np.mean(gaps > tau)

    return lockin_index, severe_prob

# -----------------------------
# Representative points
# -----------------------------
points = {
    "Stable (0.02,0.02)": (0.02, 0.02),
    "Multiplicity (0.055,0.10)": (0.055, 0.10),
    "Severe (0.07,0.15)": (0.07, 0.15),
}

# -----------------------------
# Monte Carlo
# -----------------------------
R = 50
tau = 0.25

results = {}

for name, (Delta, sigma) in points.items():
    out = {}
    for rule in ["baseline", "smoothed"]:
        vals_L = []
        vals_P = []
        for r in range(R):
            L, P = simulate_metrics(
                Delta=Delta,
                sigma=sigma,
                update_rule=rule,
                tau=tau,
                seed=1000 + r
            )
            vals_L.append(L)
            vals_P.append(P)

        out[rule] = {
            "L_mean": np.mean(vals_L),
            "L_se": np.std(vals_L, ddof=1) / np.sqrt(R),
            "P_mean": np.mean(vals_P),
            "P_se": np.std(vals_P, ddof=1) / np.sqrt(R),
        }
    results[name] = out

# -----------------------------
# Print table-ready output
# -----------------------------
print("Appendix Table C: Robustness to the regulatory update rule\n")
for name, res in results.items():
    print(name)
    for rule in ["baseline", "smoothed"]:
        r = res[rule]
        print(
            f"  {rule:9s} | "
            f"Lock-in = {r['L_mean']:.3f} ({r['L_se']:.3f}) | "
            f"Pr(severe) = {r['P_mean']:.3f} ({r['P_se']:.3f})"
        )
    print()

# -*- coding: utf-8 -*-
"""
Appendix Table C: Sensitivity to the regulatory update rule (clean, paper-aligned)

This script computes, at representative (Δ, σ) points, two statistics across Monte Carlo runs:

  1) Lock-in index: long-run average detection gap
        G = (1/(T-burn)) * sum_{t=burn}^{T-1} (max_j D_{j,t} - min_j D_{j,t})
     where D_{j,t} = θ s_{j,t}.

  2) Pr(severe lock-in): run-level probability that the long-run average gap exceeds τ
        Pr(G > τ) ≈ mean_r 1{G_r > τ}
     with τ = α θ, α = 0.2 (baseline), θ = 1.

Update rules:
  - Baseline (discrete):  s_{t+1} = Proj_{[0,1]}( s_t + Δ·1{Dhat_t < Dbar} - Δ·1{Dhat_t > Dbar} )
  - Smoothed (partial adjustment):
        s^{raw}_{t+1} = Baseline update (same as above)
        s_{t+1} = Proj_{[0,1]}( (1-ρ) s_t + ρ s^{raw}_{t+1} )
    where ρ ∈ (0,1] is a smoothing parameter (default ρ = 0.3).

Paper alignment (baseline):
  J=12, T=2000, burn=500, θ=1.0, Dbar=0.5, α=0.2 ⇒ τ=0.2, runs=40.

Notes on standard errors:
  Entries are means across runs; parentheses show standard errors (SE = sd/sqrt(R), ddof=1).
  Small SEs may round to 0.000 when printed to three decimals.
"""

from __future__ import annotations

import math
import numpy as np


def clip01(x: np.ndarray) -> np.ndarray:
    return np.clip(x, 0.0, 1.0)


def simulate_one_run(
    *,
    rng: np.random.Generator,
    update_rule: str,
    Delta: float,
    sigma: float,
    J: int,
    T: int,
    burn: int,
    theta: float,
    Dbar: float,
    tau: float,
    rho: float,
) -> tuple[float, float]:
    """
    Returns:
      G_run: long-run average gap after burn-in
      severe_run: 1{G_run > tau}
    """
    s = rng.uniform(0.0, 1.0, size=J)

    sum_gap = 0.0
    denom = max(T - burn, 1)

    for t in range(T):
        D = theta * s
        gap_t = float(D.max() - D.min())
        if t >= burn:
            sum_gap += gap_t

        # noisy observation of detection
        Dhat = D + rng.normal(0.0, sigma, size=J)

        # implied direction of discrete adjustment
        s_raw_next = clip01(
            s + Delta * (Dhat < Dbar) - Delta * (Dhat > Dbar)
        )

        if update_rule == "baseline":
            s = s_raw_next
        elif update_rule == "smoothed":
            # partial adjustment toward the implied discrete update
            s = clip01((1.0 - rho) * s + rho * s_raw_next)
        else:
            raise ValueError("update_rule must be 'baseline' or 'smoothed'")

    G_run = sum_gap / denom
    severe_run = float(G_run > tau)
    return float(G_run), float(severe_run)


def mean_and_se(x: np.ndarray) -> tuple[float, float]:
    """Return (mean, standard error) with sample sd (ddof=1)."""
    x = np.asarray(x, dtype=float)
    m = float(x.mean())
    if len(x) <= 1:
        return m, float("nan")
    se = float(x.std(ddof=1) / math.sqrt(len(x)))
    return m, se


def run_table_c(
    *,
    points: dict[str, tuple[float, float]],
    R: int,
    base_seed: int,
    J: int,
    T: int,
    burn: int,
    theta: float,
    Dbar: float,
    alpha: float,
    rho: float,
) -> dict[str, dict[str, dict[str, float]]]:
    tau = alpha * theta

    out: dict[str, dict[str, dict[str, float]]] = {}
    for label, (Delta, sigma) in points.items():
        out[label] = {}
        for rule in ("baseline", "smoothed"):
            G_vals = np.empty(R, dtype=float)
            Sev_vals = np.empty(R, dtype=float)

            # Deterministic seeding per (label, rule, run)
            # (Keeps runs comparable across rules while still independent across r.)
            for r in range(R):
                seed = base_seed + 10_000 * hash(label) % 1_000_000 + 1_000 * (rule == "smoothed") + r
                rng = np.random.default_rng(seed)
                G, sev = simulate_one_run(
                    rng=rng,
                    update_rule=rule,
                    Delta=float(Delta),
                    sigma=float(sigma),
                    J=J,
                    T=T,
                    burn=burn,
                    theta=theta,
                    Dbar=Dbar,
                    tau=tau,
                    rho=rho,
                )
                G_vals[r] = G
                Sev_vals[r] = sev

            G_mean, G_se = mean_and_se(G_vals)
            P_mean, P_se = mean_and_se(Sev_vals)

            out[label][rule] = {
                "G_mean": G_mean,
                "G_se": G_se,
                "P_mean": P_mean,
                "P_se": P_se,
            }

    return out


def print_table(results: dict[str, dict[str, dict[str, float]]]) -> None:
    print("Appendix Table C: Sensitivity to the update rule\n")
    for label, res in results.items():
        print(label)
        for rule in ("baseline", "smoothed"):
            r = res[rule]
            print(
                f"  {rule:8s} | "
                f"Lock-in index (G) = {r['G_mean']:.3f} ({r['G_se']:.3f}) | "
                f"Pr(severe) = {r['P_mean']:.3f} ({r['P_se']:.3f})"
            )
        print()


def main() -> None:
    # Representative parameter points (as in your Table C / Figure 3)
    points = {
        "Stable (0.02, 0.02)": (0.02, 0.02),
        "Multiplicity (0.055, 0.10)": (0.055, 0.10),
        "Severe (0.07, 0.15)": (0.07, 0.15),
    }

    # Paper-aligned baseline parameters (Figure 2 / Appendix A)
    J = 12
    T = 2000
    burn = 500
    theta = 1.0
    Dbar = 0.5
    alpha = 0.2  # tau = alpha * theta

    # Monte Carlo runs
    R = 40
    base_seed = 12345

    # Smoothed-rule parameter (report this in the table note)
    rho = 0.3

    results = run_table_c(
        points=points,
        R=R,
        base_seed=base_seed,
        J=J,
        T=T,
        burn=burn,
        theta=theta,
        Dbar=Dbar,
        alpha=alpha,
        rho=rho,
    )

    print_table(results)


if __name__ == "__main__":
    main()